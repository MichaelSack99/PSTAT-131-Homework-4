---
title: "Homework 4"
author: "Michael Sack"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Resampling

For this assignment, we will continue working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

![Fig. 1: RMS Titanic departing Southampton on April 10, 1912.](images/RMS_Titanic.jpg){width="363"}

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

Make sure you load the `tidyverse` and `tidymodels`!

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

Create a recipe for this dataset **identical** to the recipe you used in Homework 3.

```{r}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(yardstick)
library(corrr)
library(pROC)
library(ISLR) # For the Smarket data set
library(ISLR2) # For the Bikeshare data set
library(discrim)
library(poissonreg)
library(klaR) # for naive bayes
tidymodels_prefer()
```

```{r}
titanic <- read_csv("/Users/michellesack/Desktop/UCSB/Senior yr/Spring/PSTAT 131/homework-3/data/titanic.csv")
```

```{r}
titanic$survived <- as.factor(titanic$survived)
titanic$pclass <- as.factor(titanic$pclass)
```

### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 

```{r}
set.seed(2022)
titanic_split <- initial_split(titanic, prop = 0.80,
                                strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
dim(titanic_train)
dim(titanic_test)
dim(titanic)
titanic_train %>% 
  head()
```
```{r}
# create recipe
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>% 
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>% 
  step_center(all_numeric_predictors()) %>%
  step_scale((all_numeric_predictors()))
```

### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
```

### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?

When using *k*-fold cross-validation, we randomly assign each observation to one of the k folds. Then, it will hold out the first fold (k = 1) as the validation set and the model is then fit on the remaining k-1 folds as if they were the training set. It will compute the MSE on the observations in the hold out fold, and then continue this process for each fold. Each time this process iterates, a different fold is treated as a validation set. We should use this process because it uses every observation available. Where as, in the ordinary validation process, the training data u select does not get fitted too. We will get a better and more stable error since it is the average error among all folds. Thus, we would rather use k-fold because it is more consistent (we get a more consistent estimate).

### Question 4

Set up workflows for 3 models:

1. A logistic regression with the `glm` engine;

```{r}
# specify the model type to be logistic regression and engine to be glm
log_reg_titanic <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

# set up the workflow and fit the model to the training data
log_wkflow <- workflow() %>% 
  add_model(log_reg_titanic) %>% 
  add_recipe(titanic_recipe)
```

2. A linear discriminant analysis with the `MASS` engine;

```{r}
# specify the model type to be a linear discriminant analysis model and engine to be a 'MASS' engine
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)
```

3. A quadratic discriminant analysis with the `MASS` engine.

```{r}
# specify the model type to be a quadratic discriminant analysis model and engine to be a 'MASS' engine
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

We will be fitting 30 models total, since there are three models containing 10 folds each.

### Question 5

Fit each of the models created in Question 4 to the folded data.

**IMPORTANT:** *Some models may take a while to run – anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of [loading and saving](https://www.r-bloggers.com/2017/04/load-save-and-rda-files/). You should still include the code to run them when you knit, but set `eval = FALSE` in the code chunks.*

```{r}
fitres_log <- fit_resamples(log_wkflow,titanic_folds)
fitres_lda <- fit_resamples(lda_wkflow,titanic_folds)
fitres_qda <- fit_resamples(qda_wkflow,titanic_folds)
```

### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

```{r}
collect_metrics(fitres_log)
collect_metrics(fitres_lda)
collect_metrics(fitres_qda)
```

The logistics regression model performed best as it had the largest mean accuracy! In addition, the logistics regression model also has the smallest standard error. 

### Question 7

Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
logfit <- fit(log_wkflow, data = titanic_train)
```

### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.

```{r}
# prediction logistics regression
log_predict_test <- predict(logfit, new_data = titanic_test, type = "class")

# calculate accuracy
log_acc_test <- augment(logfit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)

accuracylog <- collect_metrics(fitres_log)$mean[1]
accuracylda <- collect_metrics(fitres_lda)$mean[1]
accuracyqda <- collect_metrics(fitres_qda)$mean[1]
mean_accuracy <- mean(c(accuracylog, accuracylda, accuracyqda))

accuracies <- c(log_acc_test$.estimate, mean_accuracy)
models <- c("Logistic Regression Accuracy", "Mean Accuracy")
results <- bind_cols(models, accuracies)
colnames(results) <- c("Model", "Accuracy")
results %>% 
  arrange(-accuracies)
```

After fitting the model to the testing data, we see that it does not have as large of an accuracy as the average across all folds of the training data.
